\documentclass{article}

\usepackage{standalone}
\usepackage{basicreq}
\usepackage{./teaching_doc_macros}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,
                chains,
                positioning,
                shapes.geometric
                }
\title{Lecture 1: Introduction}
\author{Shashank Vatedka}


\begin{document}

%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**UNIT**}{**LECTURER**}{**SCRIBE**}
\lecture{7}{Fano's Inequality}{Shashank Vatedka}{Aayush Goyal}

\section{Fano's inequality}

\textbf{If $M$ \& $\hat{M}$ are jointly distributed \7 $P_{e} = P_{r}[M\neq \hat{M}]$} then,
\begin{itemize}
    \item $H(M|\hat{M}) \leq H_{2}(P_{e}) + P_e log_{2}|M|$
\end{itemize} 

\section{Proof}

Let $E = \left\{
        \begin{array}{ll}
            1 & if \hat{M} \neq M \\
            0 & if \hat{M} \eq M
        \end{array}
    \right.$
\\ \\
$H(E) &= H_{2}(P_e)$

If $M = \hat{M} \implies H(M|\hat{M}) = 0 $

If $M, \hat{M}$ are independent $\implies H(M|\hat{M} = H(M) $
\begin{flalign}
H(M,E | \hat{M}) &= H(M|\hat{M}) + H(E|M,\hat{M}) && \\
H(M,E | \hat{M}) &= H(E|\hat{M}) + H(M|E,\hat{M}) && \\
H(M|\hat{M}) &= H(E|\hat{M}) + H(M|E, \hat{M}) - H(E|M, \hat{M}) &&
\end{flalign}
\text{Since E is a function of $M$, $\hat{M}$}
\begin{flalign}
H(M|\hat{M}) &= H(E|\hat{M}) + H(M|E, \hat{M}) - 0 &&
\end{flalign}
\text{Conditionality reduces entropy}
\begin{flalign}
H(M|\hat{M}) &\leq H(E) + H(M|E, \hat{M}) && \\
H(M|\hat{M}) &\leq  H_{2}(P_e) + H(M|E = 0, \hat{M}) P_E(0) + H(M|E = 1, \hat{M}) P_E(1) &&\\
H(M|\hat{M}) &\leq H_{2}(P_e) + H(M|\hat{M}, E=1)P_e && \\
H(M|\hat{M}) &\leq H_{2}(P_e) + P_eH(M) && \\ 
H(M|\hat{M}) &\leq H_{2}(P_e) + P_e log_{2}|M| && 
\end{flalign}

\end{document}



